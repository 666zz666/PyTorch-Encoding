Context Encoding for Semantic Segmentation (EncNet)
===================================================

Test Pre-trained Model
----------------------

- Clone the GitHub repo::
    
    git clone git@github.com:zhanghang1989/PyTorch-Encoding.git

- Install PyTorch Encoding (if not yet). Please follow the installation guide `Installing PyTorch Encoding <../notes/compile.html>`_.

- Table of pre-trained models and its performance:

    +------------+------------+-----------+-----------+-----------+-----------+
    | Method     | Backbone   | Dataset   | pixAcc    | mIoU      | Notes     |
    +============+============+===========+===========+===========+===========+
    | FCN        | ResNet50   | PASCAL12  | N/A       |           | w/o COCO  |
    +------------+------------+-----------+-----------+-----------+-----------+

Train Your Own Model
--------------------

- Prepare PASCAL VOC Dataset and Augmented Dataset::

    cd experiments/segmentation/
    bash datasets/download_datasets.sh

- Example training command for reproducing the experimensal results::

    # First training on augmented set
    CUDA_VISIBLE_DEVICES=0,1,2,3 python main.py --dataset pascal_aug --model encnet --lr 0.001 --checkname mycheckpoint
    # Finetuning on original set
    CUDA_VISIBLE_DEVICES=0,1,2,3 python main.py --dataset pascal_voc --model encnet --lr 0.0001 --checkname mycheckpoint --resume runs/pascal_aug/encnet/mycheckpoint/checkpoint.pth.tar --ft

- Detail training options::
    
  -h, --help            show this help message and exit
  --model MODEL         model name (default: encnet)
  --backbone BACKBONE   backbone name (default: resnet50)
  --dataset DATASET     dataset name (default: pascal12)
  --data-folder         training dataset folder (default: $(HOME)/data)
  --workers N           dataloader threads
  --se-loss             Semantic Encoding Loss SE-loss
  --se-loss2            Semantic Encoding Loss SE-loss 2
  --epochs N            number of epochs to train (default:100)
  --start_epoch N       start epochs (default:0)
  --batch-size N        input batch size for training (default: 16)
  --test-batch-size N   input batch size for testing (default: 32)
  --lr LR               learning rate (default: 0.01)
  --lr-scheduler        learning rate scheduler (default: poly)
  --momentum M          momentum (default: 0.9)
  --weight-decay M      weight decay (default: 1e-4)
  --no-cuda             disables CUDA training
  --seed S              random seed (default: 1)
  --resume RESUME       put the path to resuming file if needed
  --checkname CHECKNAME set the checkpoint name
  --ft                  finetuning on a different dataset
  --pre-class PRE_CLASS num of pre-trained classes (default: None)
  --eval                evaluating mIoU
  --fast-eval           fast evaluating
  --test                test a set of images and save the prediction
  --test-folder         path to test image folder


Extending the Software
----------------------

- Write your own Dataloader ``mydataset.py`` to ``datasets/`` folder

- Write your own Model ``mymodel.py`` to ``models/`` folder

- Run the program::

    python main.py --dataset mydataset --model mymodel ...
